{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ98Vf/KFRRl4/1cpPZAqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WelfLowe/RLAgents/blob/main/knapsack_with_DP_and_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knapsack problem with Dynamic Programming and Reinforcement Learning"
      ],
      "metadata": {
        "id": "gejfYfMOeS-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the simple knapsack problem, the goal is to maximize the value of items that can fit in a knapsack with a given weight capacity.\n",
        "\n",
        "We can solve this problem using:\n",
        "\n",
        "1. Dynamic Programming (DP): A classic approach to solve the knapsack problem.\n",
        "1. Reinforcement Learning (RL): Using Q-learning to find an optimal policy for item selection.\n",
        "\n",
        "**Dynamic Programming (DP)**:\n",
        "\n",
        "* Efficient and guarantees the optimal solution.\n",
        "* Requires explicit problem formulation with well-defined states and transitions.\n",
        "\n",
        "**Reinforcement Learning (RL)**:\n",
        "\n",
        "* Simulates the problem iteratively and learns through exploration and exploitation.\n",
        "* May not guarantee the optimal solution but is adaptable to problems with more complex or uncertain environments.\n",
        "\n",
        "Both methods should yield similar results for this simple problem. The RL approach, however, may show slight variations due to its stochastic nature."
      ],
      "metadata": {
        "id": "avwQrAeReBX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Description\n",
        "We have a set of items, each with a weight and value. The goal is to select a subset of items such that:\n",
        "\n",
        "The total weight does not exceed the knapsack's capacity.\n",
        "\n",
        "The total value is maximized.\n"
      ],
      "metadata": {
        "id": "Avlz59-lei_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "Items:\n",
        "* Item 0: Weight = 1, Value = 1\n",
        "* Item 1: Weight = 2, Value = 6\n",
        "* Item 2: Weight = 3, Value = 10\n",
        "* Item 3: Weight = 2, Value = 7\n",
        "\n",
        "Knapsack Capacity: 5\n",
        "\n",
        "Maximum value: 17 selecting items 2 and 3."
      ],
      "metadata": {
        "id": "4faX1ULuer63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Solution with **dynamic programming**"
      ],
      "metadata": {
        "id": "6nm8-yC5f3zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAda1ppCd10h",
        "outputId": "92b2777a-9750-4224-af9f-43b57241b758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum value: 17\n",
            "Selected items: [3, 2]\n"
          ]
        }
      ],
      "source": [
        "def knapsack_dp(values, weights, capacity):\n",
        "    n = len(values)\n",
        "    dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for w in range(capacity + 1):\n",
        "            if weights[i - 1] <= w:\n",
        "                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - weights[i - 1]] + values[i - 1])\n",
        "            else:\n",
        "                dp[i][w] = dp[i - 1][w]\n",
        "\n",
        "    # Reconstruct the solution\n",
        "    w = capacity\n",
        "    selected_items = []\n",
        "    for i in range(n, 0, -1):\n",
        "        if dp[i][w] != dp[i - 1][w]:\n",
        "            selected_items.append(i - 1)\n",
        "            w -= weights[i - 1]\n",
        "\n",
        "    return dp[n][capacity], selected_items\n",
        "\n",
        "\n",
        "# Problem definition\n",
        "values = [1, 6, 10, 7]\n",
        "weights = [1, 2, 3, 2]\n",
        "capacity = 5\n",
        "\n",
        "max_value, items = knapsack_dp(values, weights, capacity)\n",
        "print(f\"Maximum value: {max_value}\")\n",
        "print(f\"Selected items: {items}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Solution with **Q-learning**"
      ],
      "metadata": {
        "id": "OEM085X4gFZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach:\n",
        "\n",
        "* States represent the current capacity and the index of the item being considered.\n",
        "* Actions represent whether to include the current item in the knapsack or not."
      ],
      "metadata": {
        "id": "0KtudfQIgQQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def knapsack_rl(values, weights, capacity, episodes=1000, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99):\n",
        "    n = len(values)\n",
        "    q_table = np.zeros((capacity + 1, n + 1, 2))  # (remaining capacity, item index, action)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = (capacity, 0)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            cap, item = state\n",
        "\n",
        "            # Choose action (epsilon-greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice([0, 1])  # 0: skip, 1: take\n",
        "            else:\n",
        "                action = np.argmax(q_table[cap, item])\n",
        "\n",
        "            # Take action\n",
        "            if action == 1 and item < n and weights[item] <= cap:\n",
        "                next_state = (cap - weights[item], item + 1)\n",
        "                reward = values[item]\n",
        "            else:\n",
        "                next_state = (cap, item + 1)\n",
        "                reward = 0\n",
        "\n",
        "            if next_state[1] >= n:  # End of items\n",
        "                done = True\n",
        "\n",
        "            # Update Q-value\n",
        "            next_q = 0 if done else np.max(q_table[next_state[0], next_state[1]])\n",
        "            q_table[cap, item, action] += alpha * (reward + gamma * next_q - q_table[cap, item, action])\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Decay epsilon\n",
        "        epsilon = max(0.1, epsilon * epsilon_decay)\n",
        "\n",
        "    # Reconstruct the solution\n",
        "    state = (capacity, 0)\n",
        "    selected_items = []\n",
        "    while state[1] < n:\n",
        "        cap, item = state\n",
        "        action = np.argmax(q_table[cap, item])\n",
        "        if action == 1 and weights[item] <= cap:\n",
        "            selected_items.append(item)\n",
        "            state = (cap - weights[item], item + 1)\n",
        "        else:\n",
        "            state = (cap, item + 1)\n",
        "\n",
        "    return sum(values[i] for i in selected_items), selected_items\n",
        "\n",
        "\n",
        "# Problem definition\n",
        "values = [1, 6, 10, 7]\n",
        "weights = [1, 2, 3, 2]\n",
        "capacity = 5\n",
        "\n",
        "max_value, items = knapsack_rl(values, weights, capacity)\n",
        "print(f\"Maximum value: {max_value}\")\n",
        "print(f\"Selected items: {items}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0HdUtCQgKrY",
        "outputId": "d3a21644-3299-4457-8a2f-da7d748c076a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum value: 16\n",
            "Selected items: [1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taks:\n",
        "1. Understand the example\n",
        "1. Visualize the learning history of Q-learning\n",
        "1. Improve RL so that it arrives at the same result as DP in the simple example\n",
        "1. Increase the problem size and compare RL and DL"
      ],
      "metadata": {
        "id": "hM19a01ahnAy"
      }
    }
  ]
}